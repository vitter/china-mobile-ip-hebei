import aiohttp
import asyncio
import json
import ipaddress
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

# è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„dataç›®å½•
CACHE_PATH = Path(__file__).parent.parent / 'data' / 'prefixes_cache.json'
# ä½¿ç”¨RIPEstat API - å…¬å¼€ä¸”æ— éœ€è®¤è¯
API_URL = "https://stat.ripe.net/data/announced-prefixes/data.json?resource=AS{asn}"
# ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰
CACHE_EXPIRY_DAYS = 7
# API è¯·æ±‚é…ç½®
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 2  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
REQUEST_DELAY = 0.5  # è¯·æ±‚é—´å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…è§¦å‘é€Ÿç‡é™åˆ¶

def is_cache_expired(cache_data: dict) -> bool:
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
    if 'timestamp' not in cache_data:
        return True
    
    cache_time = datetime.fromtimestamp(cache_data['timestamp'])
    expiry_time = cache_time + timedelta(days=CACHE_EXPIRY_DAYS)
    is_expired = datetime.now() >= expiry_time
    
    if is_expired:
        age_days = (datetime.now() - cache_time).days
        print(f"â° ç¼“å­˜å·²è¿‡æœŸï¼ˆ{age_days} å¤©å‰åˆ›å»ºï¼Œæœ‰æ•ˆæœŸ {CACHE_EXPIRY_DAYS} å¤©ï¼‰")
    else:
        age_days = (datetime.now() - cache_time).days
        remaining_days = CACHE_EXPIRY_DAYS - age_days
        print(f"âœ“ ç¼“å­˜ä»æœ‰æ•ˆï¼ˆ{age_days} å¤©å‰åˆ›å»ºï¼Œè¿˜å‰© {remaining_days} å¤©ï¼‰")
    
    return is_expired

def load_cache() -> Dict[str, List[str]]:
    if CACHE_PATH.exists():
        try:
            content = CACHE_PATH.read_text(encoding="utf-8")
            if content.strip():
                cache_data = json.loads(content)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if is_cache_expired(cache_data):
                    print("ğŸ—‘ï¸  åˆ é™¤è¿‡æœŸç¼“å­˜")
                    CACHE_PATH.unlink()
                    return {}
                
                # è¿”å›ç¼“å­˜çš„å‰ç¼€æ•°æ®ï¼ˆä¸åŒ…æ‹¬å…ƒæ•°æ®ï¼‰
                return {k: v for k, v in cache_data.items() if k not in ['timestamp', 'version']}
        except (json.JSONDecodeError, ValueError):
            pass
    return {}

def save_cache(cache: Dict[str, List[str]]):
    CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    # æ·»åŠ æ—¶é—´æˆ³å’Œç‰ˆæœ¬ä¿¡æ¯
    cache_data = {
        'timestamp': time.time(),
        'version': '1.0',
        **cache
    }
    
    CACHE_PATH.write_text(json.dumps(cache_data, indent=2, ensure_ascii=False), encoding="utf-8")
    cache_time = datetime.fromtimestamp(cache_data['timestamp']).strftime('%Y-%m-%d %H:%M:%S')
    print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼ˆåˆ›å»ºæ—¶é—´: {cache_time}ï¼Œæœ‰æ•ˆæœŸ: {CACHE_EXPIRY_DAYS} å¤©ï¼‰")

def split_large_prefixes(prefixes: List[str], max_prefixlen: int = 24) -> List[str]:
    """
    å°†å¤§ç½‘æ®µï¼ˆæ©ç ä½æ•° < max_prefixlenï¼‰æ‹†åˆ†æˆå°ç½‘æ®µ
    
    Args:
        prefixes: CIDR åˆ—è¡¨
        max_prefixlen: æœ€å¤§æ©ç ä½æ•°ï¼Œé»˜è®¤ 24ï¼ˆä¸€ä¸ª C ç±»ç½‘æ®µï¼‰
    
    Returns:
        æ‹†åˆ†åçš„ CIDR åˆ—è¡¨
    
    Example:
        ['10.0.0.0/22'] -> ['10.0.0.0/24', '10.0.1.0/24', '10.0.2.0/24', '10.0.3.0/24']
    """
    result = []
    split_count = 0
    
    for cidr in prefixes:
        try:
            network = ipaddress.IPv4Network(cidr, strict=False)
            
            # å¦‚æœç½‘æ®µå·²ç»æ˜¯ /24 æˆ–æ›´å°ï¼Œç›´æ¥ä¿ç•™
            if network.prefixlen >= max_prefixlen:
                result.append(str(network))
            else:
                # æ‹†åˆ†æˆ /24 å­ç½‘
                subnets = list(network.subnets(new_prefix=max_prefixlen))
                result.extend([str(subnet) for subnet in subnets])
                split_count += 1
                
                # è¾“å‡ºæ‹†åˆ†ä¿¡æ¯ï¼ˆä»…å¯¹å¤§ç½‘æ®µï¼‰
                if network.prefixlen <= 20:  # åªæ˜¾ç¤º /20 åŠä»¥ä¸Šçš„å¤§ç½‘æ®µæ‹†åˆ†ä¿¡æ¯
                    print(f"  Split {cidr} -> {len(subnets)} x /{max_prefixlen} subnets")
        
        except Exception as e:
            print(f"Warning: Failed to parse {cidr}: {e}")
            result.append(cidr)  # è§£æå¤±è´¥ï¼Œä¿ç•™åŸæ ·
    
    if split_count > 0:
        print(f"âœ“ Split {split_count} large prefixes into {len(result)} subnets (/{max_prefixlen})")
    
    return sorted(set(result))

async def fetch_one(session: aiohttp.ClientSession, asn: int, semaphore: asyncio.Semaphore):
    """
    è·å–å•ä¸ª ASN çš„å‰ç¼€ï¼Œå¸¦é‡è¯•å’Œé€Ÿç‡é™åˆ¶
    """
    url = API_URL.format(asn=asn)
    
    async with semaphore:  # é™åˆ¶å¹¶å‘æ•°
        for attempt in range(MAX_RETRIES):
            try:
                # æ·»åŠ è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                if attempt > 0:
                    delay = RETRY_DELAY * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿
                    print(f"  AS{asn}: Retry {attempt}/{MAX_RETRIES} after {delay}s...")
                    await asyncio.sleep(delay)
                else:
                    await asyncio.sleep(REQUEST_DELAY)
                
                async with session.get(url, timeout=30) as r:
                    # å¤„ç†é€Ÿç‡é™åˆ¶
                    if r.status == 429:
                        retry_after = int(r.headers.get('Retry-After', RETRY_DELAY * 2))
                        print(f"âš ï¸  AS{asn}: Rate limited, waiting {retry_after}s...")
                        await asyncio.sleep(retry_after)
                        continue
                    
                    # å¤„ç†æœåŠ¡å™¨é”™è¯¯ï¼ˆ502, 503 ç­‰ï¼‰
                    if r.status in [502, 503, 504]:
                        print(f"âš ï¸  AS{asn}: Server error {r.status}, retrying...")
                        continue
                    
                    if r.status != 200:
                        print(f"âš ï¸  AS{asn}: HTTP {r.status}")
                        return str(asn), []
                    
                    data = await r.json()
                    # RIPEstat API è¿”å›æ ¼å¼: data.prefixes[].prefix
                    prefixes = []
                    if 'data' in data and 'prefixes' in data['data']:
                        for item in data['data']['prefixes']:
                            prefix = item.get('prefix', '')
                            # åªè·å–IPv4å‰ç¼€
                            if prefix and ':' not in prefix:
                                prefixes.append(prefix)
                    
                    print(f"âœ“ AS{asn}: {len(prefixes)} IPv4 prefixes")
                    return str(asn), sorted(set(prefixes))
                    
            except asyncio.TimeoutError:
                print(f"â±ï¸  AS{asn}: Timeout (attempt {attempt + 1}/{MAX_RETRIES})")
            except Exception as e:
                print(f"âŒ AS{asn}: {type(e).__name__}: {e}")
                if attempt == MAX_RETRIES - 1:
                    return str(asn), []
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        print(f"âŒ AS{asn}: Failed after {MAX_RETRIES} attempts")
        return str(asn), []

async def fetch_all(asns: List[int], use_cache=True, concurrency=5):
    """
    å¹¶å‘è·å–å¤šä¸ª ASN çš„å‰ç¼€
    
    Args:
        asns: ASN åˆ—è¡¨
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        concurrency: å¹¶å‘æ•°ï¼ˆé»˜è®¤ 5ï¼Œé¿å…è§¦å‘ API é€Ÿç‡é™åˆ¶ï¼‰
    """
    print(f"\nğŸ” Total ASNs to process: {len(asns)}")
    print(f"ğŸ“‹ Use cache: {use_cache}, Concurrency: {concurrency}")
    
    cache = load_cache() if use_cache else {}
    print(f"ğŸ’¾ Loaded cache contains {len(cache)} ASNs")
    
    tasks = []
    uncached = []

    for asn in asns:
        s = str(asn)
        if use_cache and s in cache:
            continue
        uncached.append(asn)
    
    print(f"ğŸ“Š Cached: {len(asns) - len(uncached)}, Need to fetch: {len(uncached)}")

    if not uncached:
        print("âœ“ All ASNs found in cache")
    else:
        print(f"ğŸ“¡ Fetching {len(uncached)} ASNs (concurrency: {concurrency})...")
    
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(concurrency)
    connector = aiohttp.TCPConnector(limit=concurrency * 2)
    
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [fetch_one(session, asn, semaphore) for asn in uncached]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    for item in results:
        if isinstance(item, Exception):
            continue
        asn, prefixes = item
        cache[asn] = prefixes

    # åœ¨ä¿å­˜ç¼“å­˜å‰æ‹†åˆ†å¤§ç½‘æ®µï¼Œç¡®ä¿ç¼“å­˜ä¸­ä¿å­˜çš„æ˜¯å·²æ‹†åˆ†çš„ /24 å­ç½‘
    print("\næ­£åœ¨æ‹†åˆ†å¤§ç½‘æ®µ (>=/24)...")
    total_before = sum(len(prefixes) for prefixes in cache.values())
    print(f"ğŸ“Š Prefixes before split: {total_before}")
    
    for asn_str, prefixes in cache.items():
        split_result = split_large_prefixes(prefixes)
        cache[asn_str] = split_result
    
    total_after = sum(len(prefixes) for prefixes in cache.values())
    print(f"ğŸ“Š Prefixes after split: {total_after}")
    
    save_cache(cache)

    all_prefixes = []
    for v in cache.values():
        all_prefixes.extend(v)
    
    unique_count = len(set(all_prefixes))
    print(f"ğŸ“Š Total unique prefixes to return: {unique_count}")
    
    return sorted(set(all_prefixes))

def get_prefixes_sync(asns, use_cache=True, concurrency=5):
    """
    åŒæ­¥æ–¹å¼è·å–å‰ç¼€ï¼ˆå†…éƒ¨ä½¿ç”¨å¼‚æ­¥ï¼‰
    
    Args:
        asns: ASN åˆ—è¡¨
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        concurrency: å¹¶å‘æ•°ï¼ˆé»˜è®¤ 5ï¼‰
    """
    return asyncio.run(fetch_all(asns, use_cache=use_cache, concurrency=concurrency))
