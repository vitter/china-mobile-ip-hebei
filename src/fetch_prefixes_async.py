import aiohttp
import asyncio
import json
import ipaddress
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

# è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„dataç›®å½•
CACHE_PATH = Path(__file__).parent.parent / 'data' / 'prefixes_cache.json'
# ä½¿ç”¨RIPEstat API - å…¬å¼€ä¸”æ— éœ€è®¤è¯
API_URL = "https://stat.ripe.net/data/announced-prefixes/data.json?resource=AS{asn}"
# ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰
CACHE_EXPIRY_DAYS = 7

def is_cache_expired(cache_data: dict) -> bool:
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
    if 'timestamp' not in cache_data:
        return True
    
    cache_time = datetime.fromtimestamp(cache_data['timestamp'])
    expiry_time = cache_time + timedelta(days=CACHE_EXPIRY_DAYS)
    is_expired = datetime.now() >= expiry_time
    
    if is_expired:
        age_days = (datetime.now() - cache_time).days
        print(f"â° ç¼“å­˜å·²è¿‡æœŸï¼ˆ{age_days} å¤©å‰åˆ›å»ºï¼Œæœ‰æ•ˆæœŸ {CACHE_EXPIRY_DAYS} å¤©ï¼‰")
    else:
        age_days = (datetime.now() - cache_time).days
        remaining_days = CACHE_EXPIRY_DAYS - age_days
        print(f"âœ“ ç¼“å­˜ä»æœ‰æ•ˆï¼ˆ{age_days} å¤©å‰åˆ›å»ºï¼Œè¿˜å‰© {remaining_days} å¤©ï¼‰")
    
    return is_expired

def load_cache() -> Dict[str, List[str]]:
    if CACHE_PATH.exists():
        try:
            content = CACHE_PATH.read_text(encoding="utf-8")
            if content.strip():
                cache_data = json.loads(content)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if is_cache_expired(cache_data):
                    print("ğŸ—‘ï¸  åˆ é™¤è¿‡æœŸç¼“å­˜")
                    CACHE_PATH.unlink()
                    return {}
                
                # è¿”å›ç¼“å­˜çš„å‰ç¼€æ•°æ®ï¼ˆä¸åŒ…æ‹¬å…ƒæ•°æ®ï¼‰
                return {k: v for k, v in cache_data.items() if k not in ['timestamp', 'version']}
        except (json.JSONDecodeError, ValueError):
            pass
    return {}

def save_cache(cache: Dict[str, List[str]]):
    CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    # æ·»åŠ æ—¶é—´æˆ³å’Œç‰ˆæœ¬ä¿¡æ¯
    cache_data = {
        'timestamp': time.time(),
        'version': '1.0',
        **cache
    }
    
    CACHE_PATH.write_text(json.dumps(cache_data, indent=2, ensure_ascii=False), encoding="utf-8")
    cache_time = datetime.fromtimestamp(cache_data['timestamp']).strftime('%Y-%m-%d %H:%M:%S')
    print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼ˆåˆ›å»ºæ—¶é—´: {cache_time}ï¼Œæœ‰æ•ˆæœŸ: {CACHE_EXPIRY_DAYS} å¤©ï¼‰")

def split_large_prefixes(prefixes: List[str], max_prefixlen: int = 24) -> List[str]:
    """
    å°†å¤§ç½‘æ®µï¼ˆæ©ç ä½æ•° < max_prefixlenï¼‰æ‹†åˆ†æˆå°ç½‘æ®µ
    
    Args:
        prefixes: CIDR åˆ—è¡¨
        max_prefixlen: æœ€å¤§æ©ç ä½æ•°ï¼Œé»˜è®¤ 24ï¼ˆä¸€ä¸ª C ç±»ç½‘æ®µï¼‰
    
    Returns:
        æ‹†åˆ†åçš„ CIDR åˆ—è¡¨
    
    Example:
        ['10.0.0.0/22'] -> ['10.0.0.0/24', '10.0.1.0/24', '10.0.2.0/24', '10.0.3.0/24']
    """
    result = []
    split_count = 0
    
    for cidr in prefixes:
        try:
            network = ipaddress.IPv4Network(cidr, strict=False)
            
            # å¦‚æœç½‘æ®µå·²ç»æ˜¯ /24 æˆ–æ›´å°ï¼Œç›´æ¥ä¿ç•™
            if network.prefixlen >= max_prefixlen:
                result.append(str(network))
            else:
                # æ‹†åˆ†æˆ /24 å­ç½‘
                subnets = list(network.subnets(new_prefix=max_prefixlen))
                result.extend([str(subnet) for subnet in subnets])
                split_count += 1
                
                # è¾“å‡ºæ‹†åˆ†ä¿¡æ¯ï¼ˆä»…å¯¹å¤§ç½‘æ®µï¼‰
                if network.prefixlen <= 20:  # åªæ˜¾ç¤º /20 åŠä»¥ä¸Šçš„å¤§ç½‘æ®µæ‹†åˆ†ä¿¡æ¯
                    print(f"  Split {cidr} -> {len(subnets)} x /{max_prefixlen} subnets")
        
        except Exception as e:
            print(f"Warning: Failed to parse {cidr}: {e}")
            result.append(cidr)  # è§£æå¤±è´¥ï¼Œä¿ç•™åŸæ ·
    
    if split_count > 0:
        print(f"âœ“ Split {split_count} large prefixes into {len(result)} subnets (/{max_prefixlen})")
    
    return sorted(set(result))

async def fetch_one(session: aiohttp.ClientSession, asn: int):
    url = API_URL.format(asn=asn)
    try:
        async with session.get(url, timeout=30) as r:
            if r.status != 200:
                print(f"Warning: AS{asn} returned status {r.status}")
                return str(asn), []
            
            data = await r.json()
            # RIPEstat API è¿”å›æ ¼å¼: data.prefixes[].prefix
            prefixes = []
            if 'data' in data and 'prefixes' in data['data']:
                for item in data['data']['prefixes']:
                    prefix = item.get('prefix', '')
                    # åªè·å–IPv4å‰ç¼€
                    if prefix and ':' not in prefix:
                        prefixes.append(prefix)
            
            print(f"AS{asn}: fetched {len(prefixes)} IPv4 prefixes")
            return str(asn), sorted(set(prefixes))
    except Exception as e:
        print(f"Error fetching AS{asn}: {e}")
        return str(asn), []

async def fetch_all(asns: List[int], use_cache=True, concurrency=20):
    cache = load_cache() if use_cache else {}
    tasks = []
    uncached = []

    for asn in asns:
        s = str(asn)
        if use_cache and s in cache:
            continue
        uncached.append(asn)

    connector = aiohttp.TCPConnector(limit=concurrency)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [fetch_one(session, asn) for asn in uncached]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    for item in results:
        if isinstance(item, Exception):
            continue
        asn, prefixes = item
        cache[asn] = prefixes

    # åœ¨ä¿å­˜ç¼“å­˜å‰æ‹†åˆ†å¤§ç½‘æ®µï¼Œç¡®ä¿ç¼“å­˜ä¸­ä¿å­˜çš„æ˜¯å·²æ‹†åˆ†çš„ /24 å­ç½‘
    print("\næ­£åœ¨æ‹†åˆ†å¤§ç½‘æ®µ (>=/24)...")
    for asn_str, prefixes in cache.items():
        cache[asn_str] = split_large_prefixes(prefixes)
    
    save_cache(cache)

    all_prefixes = []
    for v in cache.values():
        all_prefixes.extend(v)
    
    return sorted(set(all_prefixes))

def get_prefixes_sync(asns, use_cache=True, concurrency=20):
    return asyncio.run(fetch_all(asns, use_cache=use_cache, concurrency=concurrency))
